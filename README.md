# Real-Time Malaysian Sign Language (BIM) Recognition System

## ðŸ“– Overview
This project is a real-time Malaysian Sign Language (BIM) to text translator. It leverages computer vision and deep learning to bridge the communication gap between the deaf/hard-of-hearing community and non-signers in Malaysia.

The custom dataset consists of 15 dynamic BIM gestures, with 150 video samples per gesture, totaling 2,250 labeled sequences. All data was self-recorded and annotated based on the Malaysian Sign Language (BIM) SignBank.

Using a standard webcam, the system captures a user's **dynamic gestures** and translates them into text. Developed as a Final Year Project, this system focuses on being practical, low-cost, and accessible.

---

## Key Features
- ðŸ” **Real-Time Translation**: Converts dynamic BIM gestures to text with minimal delay.
- ðŸŽ¥ **Vision-Based**: Works with just a standard webcamâ€”no sensor gloves needed.
- âœ… **High Accuracy**: Achieved up to **98.67%** accuracy; deployed GRU model reached **97.33%**.
- ðŸ–ï¸ **15 Dynamic Gestures**: Designed for conversational signs, not just static letters.
- ðŸ–¥ï¸ **User-Friendly Interface**: Simple GUI built with **Tkinter**.

---

## âš™ï¸ Technology Stack

**Language:** Python 3.12  
**Core Libraries:**
- `TensorFlow` & `Keras` â€“ Deep learning model training & inference  
- `OpenCV` â€“ Real-time video capture and image processing  
- `MediaPipe` â€“ Real-time keypoint extraction (hands, face, pose)  
- `scikit-learn` â€“ Data splitting and evaluation  
- `Tkinter` â€“ Graphical User Interface (GUI)  
- `NumPy` â€“ Numerical data structuring

---

## â–¶ï¸ How to Run

### 1. Model Training & Evaluation (`FYP RNN.ipynb`)

This Jupyter notebook contains the full workflow:

* **Data Collection**: Captures and processes new gesture data.
* **Model Training**: Trains three RNNs â€” LSTM, BiLSTM, GRU.
* **Evaluation**: Uses accuracy/loss curves, classification reports, and confusion matrices.

> ðŸ“ Dataset should be stored in the `Action_Dataset/` directory before training.

### 2. Real-Time Application (`sign_language_gui.py`)

This is the final deployable GUI application.

#### âœ… Requirements:

* `model3.h5` â€“ Trained GRU model.
* `Action_Dataset/` â€“ Directory containing action labels.

#### ðŸ•¹ï¸ Usage:

* Click **Start** to begin detection.
* Perform BIM gestures in front of your webcam.
* Press **'q'** in the video window to stop.
* Click **Quit** to exit the app.

---

## ðŸ§  Model Architecture & Performance

* **Feature Extraction**: MediaPipe Holistic generates 1662 keypoints per frame (pose + face + hands).
* **Sequence**: Each gesture is represented by a sequence of 30 frames.
* **Dataset Split**: The dataset was split into 80% training, 10% validation, and 10% testing.

### ðŸ“Š Model Comparison

| Model    | Test Accuracy | Description                         |
| -------- | ------------- | ----------------------------------- |
| LSTM     | 94.67%        | Baseline RNN for sequences          |
| BiLSTM   | 98.22%        | Captures bidirectional context      |
| GRU      | 97.33%        | Efficient and accurate (Deployed)   |
| Ensemble | 98.67%        | Combines all three for max accuracy |

> ðŸŽ¯ **GRU** was selected for real-time deployment due to its balance of speed and performance.

---

## ðŸ“ˆ Results

### âœ… Model Evaluation

* GRU achieved **97.33% accuracy** on unseen test data.
* This model achieved a weighted average F1-score of **0.98**.
* Stable training curve, no signs of overfitting.

![Classification Report](images/classification-report-gru.png)

![Training Curves](images/training-curve.png)

![Confusion Matrices](images/confusion-matrix.png)

---

## ðŸ”® Future Work

* **Expand Vocabulary**: Add more gesture classes.
* **Mobile App**: iOS/Android deployment for accessibility.
* **Environmental Robustness**: Improve under various lighting/backgrounds.

---
